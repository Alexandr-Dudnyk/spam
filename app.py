import streamlit as st
import sklearn
import pickle
import string
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.stem import PorterStemmer

port_stemmer = PorterStemmer()

tfidf = pickle.load(open('vectorizer.pkl', 'rb'))
model = pickle.load(open('model.pkl', 'rb'))

# Create a function to generate cleaned data from raw text

def clean_text(text):
    text = word_tokenize(text) # Create tokens
    text= " ".join(text) # Join tokens
    text = [char for char in text if char not in string.punctuation] # Remove punctuations
    text = ''.join(text) # Join the leters
    text = [char for char in text if char not in re.findall(r"[0-9]", text)] # Remove Numbers
    text = ''.join(text) # Join the leters
    text = [word.lower() for word in text.split() if word.lower() not in set(stopwords.words('english'))] # Remove common english words (I, you, we,...)
    text = ' '.join(text) # Join the leters
    text = list(map(lambda x: port_stemmer.stem(x), text.split()))
    return " ".join(text)   # error word


st.title('üì© –í–∏—è–≤–ª–µ–Ω–Ω—è –°–ü–ê–ú—É')

input_sms = st.text_input("–í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —â–æ–± –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏, —á–∏ —î –≤–æ–Ω–æ —Å–ø–∞–º–æ–º.")

if st.button('–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏'):

    if input_sms == "":
        st.header('‚úçÔ∏è –í–≤–µ–¥—ñ—Ç—å –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è:')

    else:

        # 1. Preprocess
        transform_text = clean_text(input_sms)

        # 2. Vectorize
        vector_input = tfidf.transform([transform_text])

        # 3. Prediction
        result = model.predict(vector_input)

        # 4. Display

        if result == 1:
            st.header("‚ùå –¶–µ –°–ü–ê–ú")
        else:
            st.header("‚úÖ –¶–µ –Ω–µ —Å–ø–∞–º")
